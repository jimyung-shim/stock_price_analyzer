# src/stock_analyzer/news.py
from __future__ import annotations

from pathlib import Path
from datetime import datetime, timedelta
from typing import Iterable, Tuple
import time
import random
import pandas as pd
from GoogleNews import GoogleNews

# ë‚ ì§œ í¬ë§·
# yfinance ë“± ë‚´ë¶€ ë°ì´í„°ìš©: YYYY-MM-DD
DATE_FMT_ISO = "%Y-%m-%d"
# GoogleNews ë¼ì´ë¸ŒëŸ¬ë¦¬ ìš”ì²­ìš©: MM/DD/YYYY
DATE_FMT_US = "%m/%d/%Y"

def _date_range(start: datetime, end: datetime) -> Iterable[datetime]:
    """start ~ end (inclusive) í•˜ë£¨ ë‹¨ìœ„ ë°˜ë³µì."""
    cur = start
    while cur <= end:
        yield cur
        cur += timedelta(days=1)

def _fetch_daily_google_news_count(
    googlenews: GoogleNews,
    query: str,
    date: datetime
) -> int:
    """
    GoogleNews ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ íŠ¹ì • ë‚ ì§œì˜ ê¸°ì‚¬ ìˆ˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    (ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì˜ ê¸¸ì´ë¥¼ ë°˜í™˜)
    """
    date_str_us = date.strftime(DATE_FMT_US) # MM/DD/YYYY
    
    # ê²€ìƒ‰ ê¸°ê°„ ì„¤ì • (í•˜ë£¨)
    googlenews.set_time_range(date_str_us, date_str_us)
    
    # ê²€ìƒ‰ ì‹¤í–‰
    googlenews.search(query)
    
    # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    # result()ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì²« í˜ì´ì§€ì˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    # ì •í™•í•œ ì „ì²´ ê¸°ì‚¬ ìˆ˜(Total count)ëŠ” êµ¬ê¸€ì´ UIì—ì„œ ìˆ¨ê¸°ëŠ” ê²½ìš°ê°€ ë§ì•„,
    # ì—¬ê¸°ì„œëŠ” "ê²€ìƒ‰ëœ ì£¼ìš” ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ì˜ ê°œìˆ˜"ë¥¼ í™”ì œì„± ì§€í‘œë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    results = googlenews.result()
    count = len(results)
    
    # ë‹¤ìŒ ê²€ìƒ‰ì„ ìœ„í•´ ê²°ê³¼ ì´ˆê¸°í™” (í•„ìˆ˜)
    googlenews.clear()
    
    return count

def fetch_news_counts_for_ticker(
    *,
    query: str,
    start: str,
    end: str,
    out_dir: str | Path = "raw/news_data",
    sleep_min: float = 2.0,
    sleep_max: float = 5.0,
) -> Tuple[pd.DataFrame, Path]:
    """
    [start, end] êµ¬ê°„ ë™ì•ˆ í•˜ë£¨ ë‹¨ìœ„ë¡œ Google Newsë¥¼ í¬ë¡¤ë§í•˜ì—¬
    ê¸°ì‚¬ ìˆ˜ë¥¼ ì¹´ìš´íŠ¸í•˜ê³  CSVë¡œ ì €ì¥í•œë‹¤.

    Parameters
    ----------
    query : str
        ê²€ìƒ‰ í‚¤ì›Œë“œ (ì˜ˆ: "Amazon Web Services").
    start, end : str
        "YYYY-MM-DD" í˜•ì‹ì˜ ì‹œì‘/ë ë‚ ì§œ.
    out_dir : str | Path
        CSV ì €ì¥ ë””ë ‰í† ë¦¬.
    sleep_min, sleep_max : float
        êµ¬ê¸€ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ëœë¤ ëŒ€ê¸° ì‹œê°„ ë²”ìœ„ (ì´ˆ).

    Returns
    -------
    df : pandas.DataFrame
        ì»¬ëŸ¼: [date, query, count]
    out_path : pathlib.Path
        ì €ì¥ëœ CSV íŒŒì¼ ê²½ë¡œ.
    """
    
    # GoogleNews ê°ì²´ ì´ˆê¸°í™” (ì–¸ì–´: ì˜ì–´, ì§€ì—­: ë¯¸êµ­)
    googlenews = GoogleNews(lang='en', region='US')
    # ì¸ì½”ë”© ì„¤ì • (ê°€ë” ê¹¨ì§€ëŠ” ë¬¸ì œ ë°©ì§€)
    googlenews.set_encode('utf-8')

    start_dt = datetime.strptime(start, DATE_FMT_ISO)
    end_dt = datetime.strptime(end, DATE_FMT_ISO)

    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    
    # íŒŒì¼ëª… ë¯¸ë¦¬ ìƒì„±
    safe_query = query.replace(" ", "_").replace("/", "_")
    filename = f"{safe_query}_news_counts_{start}_to_{end}.csv"
    out_path = out_dir / filename

    # ì´ë¯¸ íŒŒì¼ì´ ìˆë‹¤ë©´ ë¡œë“œí•´ì„œ ì¤‘ë‹¨ëœ ì§€ì ë¶€í„° ì´ì–´í•˜ê¸° (Resumable)
    if out_path.exists():
        print(f"Found existing file: {out_path}. Resuming...")
        df_exist = pd.read_csv(out_path)
        records = df_exist.to_dict("records")
        # ë§ˆì§€ë§‰ ë‚ ì§œ í™•ì¸
        if not df_exist.empty:
            last_date_str = df_exist.iloc[-1]["date"]
            last_date = datetime.strptime(last_date_str, DATE_FMT_ISO)
            # ì‹œì‘ì¼ì„ ë§ˆì§€ë§‰ ê¸°ë¡ ë‹¤ìŒ ë‚ ë¡œ ì¡°ì •
            start_dt = last_date + timedelta(days=1)
    else:
        records = []

    print(f"ğŸ” Starting crawl for '{query}' from {start_dt.date()} to {end_dt.date()}")
    
    try:
        for d in _date_range(start_dt, end_dt):
            d_str = d.strftime(DATE_FMT_ISO)
            
            try:
                count = _fetch_daily_google_news_count(googlenews, query, d)
            except Exception as e:
                print(f"âš ï¸ Error on {d_str}: {e}")
                count = 0 # ì—ëŸ¬ ì‹œ 0ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ì§„í–‰
                
                # ì—ëŸ¬ ë°œìƒ ì‹œ ì¡°ê¸ˆ ë” ê¸¸ê²Œ ëŒ€ê¸°
                time.sleep(10) 

            print(f"   [{d_str}] found: {count} articles")
            
            records.append({
                "date": d_str,
                "query": query,
                "count": count,
            })

            # ì¤‘ê°„ ì €ì¥ (ë°ì´í„° ìœ ì‹¤ ë°©ì§€)
            if len(records) % 10 == 0:
                pd.DataFrame(records).to_csv(out_path, index=False)

            # ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ëœë¤ ìŠ¬ë¦½
            sleep_time = random.uniform(sleep_min, sleep_max)
            time.sleep(sleep_time)

    except KeyboardInterrupt:
        print("\nğŸ›‘ Crawling interrupted by user. Saving progress...")
    
    # ìµœì¢… ì €ì¥
    df = pd.DataFrame(records)
    df.to_csv(out_path, index=False)
    
    print(f"âœ… Saved news data to: {out_path}")
    return df, out_path