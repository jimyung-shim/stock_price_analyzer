# src/stock_analyzer/news.py
from __future__ import annotations

from pathlib import Path
from datetime import datetime, timedelta
from typing import Iterable, Tuple
import time
import random
import pandas as pd
from GoogleNews import GoogleNews

# ë‚ ì§œ í¬ë§·
DATE_FMT_ISO = "%Y-%m-%d"
# GoogleNews ë¼ì´ë¸ŒëŸ¬ë¦¬ ìš”ì²­ìš©: MM/DD/YYYY
DATE_FMT_US = "%m/%d/%Y"

def _date_range(start: datetime, end: datetime) -> Iterable[datetime]:
    """start ~ end (inclusive) í•˜ë£¨ ë‹¨ìœ„ ë°˜ë³µì."""
    cur = start
    while cur <= end:
        yield cur
        cur += timedelta(days=1)

def _fetch_daily_google_news_count(
    googlenews: GoogleNews,
    query: str,
    date: datetime
) -> int:
    """
    íŠ¹ì • ë‚ ì§œì˜ ê¸°ì‚¬ ìˆ˜ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ í˜ì´ì§€ë¥¼ ë„˜ê¸°ë©° ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    ì‹œê°„ ì ˆì•½ ë° ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•´ ìµœëŒ€ 5í˜ì´ì§€(ì•½ 50ê°œ)ê¹Œì§€ë§Œ í™•ì¸í•©ë‹ˆë‹¤.
    """
    date_str_us = date.strftime(DATE_FMT_US) # MM/DD/YYYY
    
    # ê¸°ê°„ ì„¤ì • ë° ì´ˆê¸° ê²€ìƒ‰
    googlenews.set_time_range(date_str_us, date_str_us)
    googlenews.search(query)
    
    # ì²« í˜ì´ì§€ ê²°ê³¼ ìˆ˜ í™•ì¸
    try:
        results = googlenews.result()
        count = len(results)
    except Exception:
        # ê²€ìƒ‰ ê²°ê³¼ ìì²´ê°€ ì—ëŸ¬ì¸ ê²½ìš°
        googlenews.clear()
        return 0
    
    # ì²« í˜ì´ì§€ê°€ 10ê°œ ë¯¸ë§Œì´ë©´ ë” ë³¼ í•„ìš” ì—†ìŒ (ê·¸ê²Œ ì „ì²´ ê°œìˆ˜ì„)
    if count < 10:
        googlenews.clear()
        return count

    # ê¸°ì‚¬ê°€ ë§ì„ ê²½ìš° 2~5í˜ì´ì§€ê¹Œì§€ ì¶”ê°€ íƒìƒ‰
    max_pages = 5 
    
    for page in range(2, max_pages + 1):
        try:
            # [ìˆ˜ì •ë¨] í˜ì´ì§€ ë„˜ê¸¸ ë•Œ ëŒ€ê¸° ì‹œê°„ ëŒ€í­ ì¦ê°€ (0.5ì´ˆ -> 3~5ì´ˆ ëœë¤)
            time.sleep(random.uniform(3.0, 5.0))
            
            googlenews.get_page(page)
            new_results = googlenews.result()
            new_count = len(new_results)
            
            if new_count == count:
                break
            
            count = new_count
            
        except Exception:
            break
            
    googlenews.clear()
    return count

def fetch_news_counts_for_ticker(
    *,
    query: str,
    start: str,
    end: str,
    out_dir: str | Path = "raw/news_data",
    # [ìˆ˜ì •ë¨] ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ ëŒ€í­ ì¦ê°€ (ê¸°ì¡´ 1.5~3.0 -> 6.0~10.0)
    sleep_min: float = 6.0,
    sleep_max: float = 12.0,
) -> Tuple[pd.DataFrame, Path]:
    """
    Google Newsë¥¼ í¬ë¡¤ë§í•˜ì—¬ ì¼ë³„ ê¸°ì‚¬ ìˆ˜(Trend)ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    """
    
    # GoogleNews ê°ì²´ ì´ˆê¸°í™”
    googlenews = GoogleNews(lang='en', region='US')
    googlenews.set_encode('utf-8')

    start_dt = datetime.strptime(start, DATE_FMT_ISO)
    end_dt = datetime.strptime(end, DATE_FMT_ISO)

    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    
    safe_query = query.replace(" ", "_").replace("/", "_")
    filename = f"{safe_query}_news_counts_{start}_to_{end}.csv"
    out_path = out_dir / filename

    records = []
    # ì´ì–´í•˜ê¸° ë¡œì§
    if out_path.exists():
        try:
            print(f"ğŸ“‚ Found existing file: {out_path}. Checking last date...")
            df_exist = pd.read_csv(out_path)
            if not df_exist.empty:
                last_date_str = df_exist.iloc[-1]["date"]
                last_date = datetime.strptime(last_date_str, DATE_FMT_ISO)
                if last_date >= start_dt:
                    start_dt = last_date + timedelta(days=1)
                    records = df_exist.to_dict("records")
                    print(f"â­ï¸  Resuming from {start_dt.date()}...")
        except Exception as e:
            print(f"âš ï¸ Error reading existing file: {e}. Starting fresh.")
            records = []

    if start_dt > end_dt:
        print("âœ… All data already collected.")
        return pd.DataFrame(records), out_path

    print(f"ğŸ” Starting Slow & Safe crawl for '{query}' from {start_dt.date()} to {end_dt.date()}")
    
    try:
        for i, d in enumerate(_date_range(start_dt, end_dt)):
            d_str = d.strftime(DATE_FMT_ISO)
            
            # [ì¶”ê°€] 10ì¼ë§ˆë‹¤ í•œ ë²ˆì”© ì•„ì£¼ ê¸¸ê²Œ ì‰¬ê¸° (30ì´ˆ)
            if i > 0 and i % 10 == 0:
                print("â˜• Taking a long coffee break (30s) to avoid detection...")
                time.sleep(30)

            try:
                count = _fetch_daily_google_news_count(googlenews, query, d)
            except Exception as e:
                print(f"âš ï¸ Error on {d_str}: {e}")
                # 429 ì—ëŸ¬ ë°œìƒ ì‹œ 1ë¶„ê°„ ëŒ€ê¸° í›„ 0 ì²˜ë¦¬ (ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°)
                time.sleep(60)
                count = 0 

            print(f"   [{d_str}] found: {count} articles")
            
            records.append({
                "date": d_str,
                "query": query,
                "count": count,
            })

            if len(records) % 5 == 0:
                pd.DataFrame(records).to_csv(out_path, index=False)

            # ì¼ì¼ ìˆ˜ì§‘ ê°„ ëŒ€ê¸° ì‹œê°„ (ëœë¤ 6~12ì´ˆ)
            time.sleep(random.uniform(sleep_min, sleep_max))

    except KeyboardInterrupt:
        print("\nğŸ›‘ Crawling interrupted by user. Saving progress...")
    
    df = pd.DataFrame(records)
    df.to_csv(out_path, index=False)
    
    print(f"âœ… Saved news data to: {out_path}")
    return df, out_path