# src/stock_analyzer/news.py
from __future__ import annotations

from pathlib import Path
from datetime import datetime, timedelta
from typing import Iterable, Tuple
import time
import random
import pandas as pd
from GoogleNews import GoogleNews

# ë‚ ì§œ í¬ë§·
DATE_FMT_ISO = "%Y-%m-%d"
# GoogleNews ë¼ì´ë¸ŒëŸ¬ë¦¬ ìš”ì²­ìš©: MM/DD/YYYY
DATE_FMT_US = "%m/%d/%Y"

def _date_range(start: datetime, end: datetime) -> Iterable[datetime]:
    """start ~ end (inclusive) í•˜ë£¨ ë‹¨ìœ„ ë°˜ë³µì."""
    cur = start
    while cur <= end:
        yield cur
        cur += timedelta(days=1)

def _fetch_daily_google_news_count(
    googlenews: GoogleNews,
    query: str,
    date: datetime
) -> int:
    """
    íŠ¹ì • ë‚ ì§œì˜ ê¸°ì‚¬ ìˆ˜ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ í˜ì´ì§€ë¥¼ ë„˜ê¸°ë©° ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    ì‹œê°„ ì ˆì•½ ë° ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•´ ìµœëŒ€ 5í˜ì´ì§€(ì•½ 50ê°œ)ê¹Œì§€ë§Œ í™•ì¸í•©ë‹ˆë‹¤.
    """
    date_str_us = date.strftime(DATE_FMT_US) # MM/DD/YYYY
    
    # ê¸°ê°„ ì„¤ì • ë° ì´ˆê¸° ê²€ìƒ‰
    googlenews.set_time_range(date_str_us, date_str_us)
    googlenews.search(query)
    
    # ì²« í˜ì´ì§€ ê²°ê³¼ ìˆ˜ í™•ì¸
    results = googlenews.result()
    count = len(results)
    
    # ì²« í˜ì´ì§€ê°€ 10ê°œ ë¯¸ë§Œì´ë©´ ë” ë³¼ í•„ìš” ì—†ìŒ (ê·¸ê²Œ ì „ì²´ ê°œìˆ˜ì„)
    if count < 10:
        googlenews.clear()
        return count

    # ê¸°ì‚¬ê°€ ë§ì„ ê²½ìš° 2~5í˜ì´ì§€ê¹Œì§€ ì¶”ê°€ íƒìƒ‰ (ìµœëŒ€ 50ê°œê¹Œì§€ ì¹´ìš´íŠ¸)
    # 3ë…„ì¹˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ì•¼ í•˜ë¯€ë¡œ, ì†ë„ë¥¼ ìœ„í•´ 5í˜ì´ì§€ë¡œ ì œí•œí•˜ëŠ” ê²ƒì´ í˜„ì‹¤ì ì…ë‹ˆë‹¤.
    max_pages = 5 
    
    for page in range(2, max_pages + 1):
        try:
            googlenews.get_page(page)
            new_results = googlenews.result()
            new_count = len(new_results)
            
            # ë” ì´ìƒ ê¸°ì‚¬ê°€ ëŠ˜ì–´ë‚˜ì§€ ì•Šìœ¼ë©´(ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬) ì¤‘ë‹¨
            if new_count == count:
                break
            
            count = new_count
            
            # í˜ì´ì§€ ë„˜ê¸¸ ë•Œë§ˆë‹¤ ì•„ì£¼ ì§§ì€ ëŒ€ê¸° (ê¸°ê³„ì  ì ‘ê·¼ ë°©ì§€)
            time.sleep(0.5)
            
        except Exception:
            # í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨ ì‹œ í˜„ì¬ê¹Œì§€ ì¹´ìš´íŠ¸ ë°˜í™˜
            break
            
    # ë‹¤ìŒ ë‚ ì§œë¥¼ ìœ„í•´ ê²°ê³¼ ì´ˆê¸°í™” (í•„ìˆ˜)
    googlenews.clear()
    return count

def fetch_news_counts_for_ticker(
    *,
    query: str,
    start: str,
    end: str,
    out_dir: str | Path = "raw/news_data",
    sleep_min: float = 1.5,
    sleep_max: float = 3.0,
) -> Tuple[pd.DataFrame, Path]:
    """
    Google Newsë¥¼ í¬ë¡¤ë§í•˜ì—¬ ì¼ë³„ ê¸°ì‚¬ ìˆ˜(Trend)ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    """
    
    # GoogleNews ê°ì²´ ì´ˆê¸°í™”
    googlenews = GoogleNews(lang='en', region='US')
    googlenews.set_encode('utf-8')

    start_dt = datetime.strptime(start, DATE_FMT_ISO)
    end_dt = datetime.strptime(end, DATE_FMT_ISO)

    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    
    safe_query = query.replace(" ", "_").replace("/", "_")
    filename = f"{safe_query}_news_counts_{start}_to_{end}.csv"
    out_path = out_dir / filename

    # ì´ì–´í•˜ê¸°(Resume) ê¸°ëŠ¥
    records = []
    if out_path.exists():
        try:
            print(f"ğŸ“‚ Found existing file: {out_path}. Checking last date...")
            df_exist = pd.read_csv(out_path)
            if not df_exist.empty:
                last_date_str = df_exist.iloc[-1]["date"]
                last_date = datetime.strptime(last_date_str, DATE_FMT_ISO)
                if last_date >= start_dt:
                    start_dt = last_date + timedelta(days=1)
                    records = df_exist.to_dict("records")
                    print(f"â­ï¸  Resuming from {start_dt.date()}...")
        except Exception as e:
            print(f"âš ï¸ Error reading existing file: {e}. Starting fresh.")
            records = []

    if start_dt > end_dt:
        print("âœ… All data already collected.")
        return pd.DataFrame(records), out_path

    print(f"ğŸ” Starting deep crawl for '{query}' from {start_dt.date()} to {end_dt.date()}")
    print("   (Checking up to 5 pages per day to capture trends...)")
    
    try:
        for d in _date_range(start_dt, end_dt):
            d_str = d.strftime(DATE_FMT_ISO)
            
            try:
                count = _fetch_daily_google_news_count(googlenews, query, d)
            except Exception as e:
                print(f"âš ï¸ Error on {d_str}: {e}")
                count = 0 
                time.sleep(5) # ì—ëŸ¬ ì‹œ ì ì‹œ ëŒ€ê¸°

            print(f"   [{d_str}] found: {count} articles")
            
            records.append({
                "date": d_str,
                "query": query,
                "count": count,
            })

            # ë°ì´í„° ìœ ì‹¤ ë°©ì§€ë¥¼ ìœ„í•´ 5ì¼ë§ˆë‹¤ ì €ì¥
            if len(records) % 5 == 0:
                pd.DataFrame(records).to_csv(out_path, index=False)

            # ì°¨ë‹¨ ë°©ì§€ ëŒ€ê¸°
            time.sleep(random.uniform(sleep_min, sleep_max))

    except KeyboardInterrupt:
        print("\nğŸ›‘ Crawling interrupted by user. Saving progress...")
    
    # ìµœì¢… ì €ì¥
    df = pd.DataFrame(records)
    df.to_csv(out_path, index=False)
    
    print(f"âœ… Saved news data to: {out_path}")
    return df, out_path